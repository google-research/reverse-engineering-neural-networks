{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart\n",
    "\n",
    "This notebook walks through some of the basic functionality provided by the `renn` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nirum/anaconda3/lib/python3.8/site-packages/jax/lib/xla_bridge.py:130: UserWarning: No GPU/TPU found, falling back to CPU.\n",
      "  warnings.warn('No GPU/TPU found, falling back to CPU.')\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from functools import partial\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import renn\n",
    "\n",
    "base_key = jax.random.PRNGKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and train RNNs\n",
    "\n",
    "First, we will use the provided RNN cell classes to build different RNN architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Made a GRU cell with 32 units.\n"
     ]
    }
   ],
   "source": [
    "# Here, we build an RNN composed of a single GRU cell.\n",
    "cell = renn.GRU(32)\n",
    "print(f'Made a GRU cell with {cell.num_units} units.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can initialize the hidden state for this cell as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized state with shape: (32,)\n"
     ]
    }
   ],
   "source": [
    "key, base_key = jax.random.split(base_key)\n",
    "current_state = cell.init_initial_state(key)\n",
    "print(f'Initialized state with shape: {current_state.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can initialize the cell's trainable parameters using cell.init:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs have shape: (100, 32)\n"
     ]
    }
   ],
   "source": [
    "num_timesteps = 100\n",
    "input_dim = 2\n",
    "input_shape = (num_timesteps, input_dim)\n",
    "\n",
    "key, base_key = jax.random.split(base_key)\n",
    "output_shape, params = cell.init(key, input_shape)\n",
    "\n",
    "print(f'Outputs have shape: {output_shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GRU cell is a subclass of RNNCell. All RNNCells have an apply method that computes a single RNN step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next state has shape: (32,)\n"
     ]
    }
   ],
   "source": [
    "key, base_key = jax.random.split(base_key)\n",
    "inputs = jax.random.normal(key, (input_dim,))\n",
    "\n",
    "next_state = cell.apply(params, inputs, current_state)\n",
    "print(f'Next state has shape: {next_state.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply the RNN across an entire batch of sequences, we use the renn.unroll_rnn function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied RNN to a batch of sequences, got back states with shape: (8, 100, 32)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "key, base_key = jax.random.split(base_key)\n",
    "batched_inputs = jax.random.normal(key, (batch_size,) + input_shape)\n",
    "batch_initial_states = cell.get_initial_state(params, batch_size=batch_size)\n",
    "\n",
    "states = renn.unroll_rnn(batch_initial_states, batched_inputs, partial(cell.batch_apply, params))\n",
    "\n",
    "print(f'Applied RNN to a batch of sequences, got back states with shape: {states.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use these to train RNNs on different kinds of sequential data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing RNNs\n",
    "\n",
    "The RNN cells we have in renn are easily amenable for analysis. One useful tool is to _linearize_ the RNN, meaning we compute a first-order (linear) Taylor approximation of the _nonlinear_ RNN update.\n",
    "\n",
    "Mathematically, we can approximate the RNN at a particular expansion point ($h$, $x$) as follows:\n",
    "$$ F(h + \\Delta h, x + \\Delta x) \\approx h + \\frac{\\partial F}{\\partial h} \\left(\\Delta h\\right) + \\frac{\\partial F}{\\partial x} \\left(\\Delta x\\right) $$\n",
    "\n",
    "In the above equation, the term $\\frac{\\partial F}{\\partial h}$ is the _recurrent Jacobian_ of the RNN, and the term $\\frac{\\partial F}{\\partial x}$ is the _input Jacobian_.\n",
    "\n",
    "We can easily compute Jacobians of our GRU cell at a particular point. We can do this using the `rec_jac` and `inp_jac` methods on the cell class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recurrent Jacobian has shape: (32, 32)\n",
      "Input Jacobian has shape: (32, 2)\n"
     ]
    }
   ],
   "source": [
    "Jacobian = cell.rec_jac(params, inputs, current_state)\n",
    "print(f'Recurrent Jacobian has shape: {Jacobian.shape}')\n",
    "\n",
    "Jacobian = cell.inp_jac(params, inputs, current_state)\n",
    "print(f'Input Jacobian has shape: {Jacobian.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`renn` also contains helper functions for numerically finding fixed points of the RNN, for building and training different RNN architectures, and for training and analyzing RNN optimizers.\n",
    "\n",
    "In future tutorials, we will explore some of these additional use cases!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
